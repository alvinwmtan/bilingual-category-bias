---
title: |
  | The origins of syntactic category biases: 
  | Evidence from early vocabularies of bilingual children
bibliography: library.bib
csl: apa7.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Alvin Wei Ming Tan (tanawm@stanford.edu)} \\ Department of Psychology \\ Stanford University
    \AND {\large \bf Michael C. Frank} \\ Department of Psychology \\ Stanford University}
    
abstract: >
    Why are nouns often over-represented in children's early words? Prior research has suggested a range of possible explanations for the observed bias in the syntactic category composition of early vocabularies, including cognitive, linguistic, and contextual factors. However, these factors are not mutually exclusive and can be difficult to disambiguate empirically. Across three analyses of the vocabularies of children acquiring two languages ($N$ = 1997), we investigated the role of different language combinations, different levels of language exposure, and different environmental contexts, finding some support for cross-linguistic modulation of syntactic category bias, as well as variation across countries. These results suggest that both linguistic and contextual factors contribute to syntactic category biases in young children's vocabularies, and highlight the importance of understanding the broader background of the child when studying language acquisition.
    
keywords: >
    bilingualism; vocabulary; noun bias
    
output: cogsci2024::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options}
#| include: FALSE
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r}
#| message: FALSE
library(tidyverse)
library(here)
library(glue)
library(patchwork)

source("scripts/analysis.R")
source("scripts/plots.R")

INTERMEDIATE_LOC <- here("intermediates")
```

# Introduction
Children's vocabularies reflect the factors that shape their language learning, including language input, learning environments, and cognitive biases.
One way to analyse such early vocabularies is to examine their composition in terms of syntactic categories.
Early vocabularies are dominated by words for objects, rather than words for relations; this is known as the noun bias, and has been reported for a wide range of languages [e.g., @bornsteinCrossLinguisticAnalysisVocabulary2004; @frankVariabilityConsistencyEarly2021; @gentnerWhyNounsAre1982; see @waxmanAreNounsLearned2013 for a review].
Nonetheless, there has also been evidence suggesting that some languages exhibit no such noun bias, including Mandarin [@tardifNounsAreNot1996], Korean [@choiEarlyAcquisitionVerbs1995], and Tseltal [@casillasLittleEvidenceNoun2024] among others.
Furthermore, different studies have found conflicting results even for the same language (e.g., compare @bornsteinCrossLinguisticAnalysisVocabulary2004 and @choiEarlyAcquisitionVerbs1995 for Korean).
The observed variation in results bear upon theoretical explanations for the occurrence of the noun bias in some (but probably not all) early vocabularies.

Hypothesised accounts for the observed noun bias fall into three broad classes: cognitive, linguistic, or contextual.
The first focuses on the _cognitive_ process of word learning, proposing that nouns are easier to encode than verbs.
Gentner's [-@gentnerWhyNounsAre1982] natural partitions/relational relativity hypothesis suggests that objects are perceptually individuable (thus easier to map to labels), whereas relations are individuated only via linguistic selection (thus are harder to learn).

The second class relates to _linguistic_ features of nouns and verbs.
Such explanations include the suggestion that, relative to verbs, nouns are more frequent in child-directed speech [e.g., @goldfieldNounBiasMaternal1993; @kimEarlyLexicalDevelopment2000], less morphologically complex [e.g., @tardifCaregiverSpeechChildrens1997], or more salient [e.g., @caselliCrosslinguisticStudyEarly1995].
Crucially, this class seeks to account for the observed variation in the noun bias across languages, since the posited features vary across languages.

The third class is _contextual_, suggesting that sociocultural features influence how nouns and verbs are communicated about or processed.
For example, cultures may emphasise labels to differing extents [e.g., @choiEarlyAcquisitionVerbs1995; @tardifPuttingNounBias1999], or may have differential patterns of attention towards objects or relations [e.g., @lavinEastWestRole2006; @waxmanHowEarlyInfants2016].

Arbitrating among these classes of explanations can shed light on the mechanisms of language learning---e.g., whether there are universal learning biases.
Notably, these explanations are not mutually exclusive, and multiple sets of features may jointly influence the learning of words across syntactic categories.
However, it is typically difficult to distinguish between linguistic and contextual factors, since they tend to covary---for example, case studies demonstrating lower noun bias in East Asian languages confound cultural and linguistic factors when comparing with North American English.

One way to disentangle linguistic and contextual factors is by studying bilingual children, who serve as a "natural experiment", since they reside within a single context while acquiring two languages, i.e., contextual factors are constant[^0] while linguistic factors vary across their two languages.
Previous research have shown that bilingual children exhibit differently sized noun biases in their two languages.
This effect has been attested in English--Mandarin bilinguals [e.g., <!--@chanPredictingTwoMandarinEnglish2010;--> @leveyFirstWordsProduced2003; <!--@setohContrastingLexicalBiases2021;--> @xinNounVerbBias2010; @xuanLanguagespecificNounBias2013], Filipino--English bilinguals [@lucasExploringNounBias2008] and Turkish--Dutch bilinguals [@ozcanEarlyLexicalComposition2016], although other language combinations did not find this effect [@klammlerSizeCompositionProductive2011; @nicoladisEvidenceBilingualChild2001].
<!-- The language-specific bias may depend on the specific language combination, however, as no such effect was found for a Portuguese--English bilingual child [@nicoladisEvidenceBilingualChild2001] or a German--Italian bilingual child [@klammlerSizeCompositionProductive2011]---although it should be noted that these are case studies and may not be generalisable or well-powered. -->
Additionally, @chaiExtralinguisticModulationEnglish2021 has shown that bilinguals learning different language combinations (English--Mandarin vs English--Malay) can show differently sized noun biases in the common language (in this case, English), suggesting that there is some cross-linguistic influence, perhaps due to the preferential learning of translation equivalents [e.g., "dog" in English and "perro" in Spanish\; see @tanRoleTranslationEquivalents2024; @tsuiAreTranslationEquivalents2022].
Different levels of language exposure have also been shown to affect noun bias sizes in Basque--Spanish bilingual children [@barnesVocabularyGrowthComposition2013].
Together, these results suggest that linguistic (and cross-linguistic) factors must play a role in explaining the noun bias, since variation across languages persists even when cognition and context are controlled for.

[^0]: Note that contextual factors may be constant within a bilingual child, but may be different than monolinguals in the same country.

Studies of the noun bias vary widely in how they analyse their data, which could also explain variation in their results.
Some studies used direct counts of the number of nouns or verbs known by a child [e.g., <!--@goldfieldNounsVerbsComprehension2000; -->@leveyFirstWordsProduced2003; <!--@lucasExploringNounBias2008; -->@ozcanEarlyLexicalComposition2016;<!-- @xinNounVerbBias2010;--> @tardifCaregiverSpeechChildrens1997].
Other studies measured nouns or verbs known as a proportion of the child's vocabulary [e.g., <!--@barnesVocabularyGrowthComposition2013; -->@bornsteinCrossLinguisticAnalysisVocabulary2004; <!--@caselliCrosslinguisticStudyEarly1995; -->@chanPredictingTwoMandarinEnglish2010; @choiEarlyAcquisitionVerbs1995;<!-- @xuanLanguagespecificNounBias2013;--> @gentnerWhyNounsAre1982].
A third option relies on ratios---for example, the ratio of nouns to the sum of nouns and verbs [e.g., @chaiExtralinguisticModulationEnglish2021; @setohContrastingLexicalBiases2021; @tardifPuttingNounBias1999].
However, each of these approaches is potentially affected by an availability problem: if there are more nouns than verbs that are _available_ for a child to learn, we would expect them to have more nouns in their vocabulary (whether by count, proportion, or ratio) even if they were acquiring words entirely by chance.
In other words, when evaluating whether children have tendencies to prefer learning nouns over verbs, our key metric of interest should be whether children know more nouns _than would be expected_---i.e., whether there is an _over-representation_ of nouns in the child's vocabulary.

To study the over- or under-representation of syntactic categories in early bilingual vocabularies, we adopted the approach taken by @batesDevelopmentalStylisticVariation1994 and @frankVariabilityConsistencyEarly2021. 
This approach used data from parent-report vocabulary checklists, and children's known words are expressed as proportions of the total number of words in that category on the checklist (i.e., potential opportunities for word learning).
This is then plotted against children's total vocabularies, again expressed as proportions of the total size of the checklist; a schematic is shown in Figure \ref{fig:schematic}.
If children were to acquire words independently of their syntactic category, then we would expect their category proportions to match their total vocabulary proportions (i.e., no bias)<!-----for example, if a child knows 50% of the words on the checklist, they should also know 50% of the nouns, 50% of the predicates, and 50% of the function words-->. 
Conversely, if children had a positive category bias, then their category proportions should exceed their total vocabulary proportions, suggesting an over-representation of nouns in their vocabularies, and vice versa for negative category bias and under-representation. 
Such an analytic approach would thus control for the word learning opportunities that children experience overall, allowing the specific investigation of whether there are learning biases beyond mere variation in the availability of learnable words.

```{r schematic}
#| fig.env = "figure", 
#| fig.pos = "t", 
#| fig.align = "center", 
#| fig.height = 2.5,
#| fig.width = 5.5,
#| out.width = "240pt",
#| set.cap.width = T, 
#| num.cols.cap = 1, 
#| fig.cap = "A schematic of possible bias values for a given syntactic category."

schematic_df <- expand_grid(
  prop_total = pts,
  lex_cat = c("over-represented", "no bias", "under-represented")
) |> 
  mutate(prop_class = case_when(
    lex_cat == "over-represented" ~ (prop_total - 1) ^ 3 + 1,
    lex_cat == "under-represented" ~ prop_total ^ 3,
    lex_cat == "no bias" ~ prop_total
  ),
  lex_cat = factor(lex_cat, levels = c("over-represented", "no bias", "under-represented")))
make_prop_plot(NULL, schematic_df, NULL) +
  scale_color_manual(values = c("#c4c4c4", "#808080", "#363636")) +
  labs(col = "Syntactic category bias") +
  theme(legend.position = "right")
```

The vocabulary checklist used by @batesDevelopmentalStylisticVariation1994 and @frankVariabilityConsistencyEarly2021 (and in the present study) is the Communicative Development Inventory [CDI, @marchmanMacArthurBatesCommunicativeDevelopment2023].
CDIs have been used by researchers in many contexts due to their ease of administration, reliability and validity as a measure of language ability, and availability in many language- and context-appropriate adaptations [@fensonMacArthurBatesCommunicativeDevelopment2007; @mayorStatisticalEstimateInfant2011].
Note that collecting bilingual CDI data presents a unique set of challenges [@weislederUsingParentReport2024], not least because caregivers may be unequally proficient in the two languages<!--, resulting in potential under-reporting for their less proficient language--> [@coreParentLanguageProficiency2024].
Hence, care must be taken when interpreting the relationship between language exposure and children's vocabularies due to possible measurement error.
Nonetheless, CDIs have been validated as tool for measuring bilingual children's vocabularies [@weislederUsingParentReport2024], and provide a plausible metric for words that may be learnable by young children.

The present study employs the over-/under-representation approach on CDI data to study syntactic category biases in bilingual children, aiming to disentangle the roles of linguistic and contextual factors in shaping early vocabulary compositions.
Following @frankVariabilityConsistencyEarly2021, we investigated not only nouns and predicates (verbs and adjectives), but also function words, which have larger absolute bias sizes (and may therefore have larger, more detectable effect sizes).
We consider the following hypotheses, which are not mutually exclusive:

> H1: The category bias is affected by linguistic factors (e.g., frequency or morphosyntax). Hence, bilingual children have different-sized category biases across their two languages.

> H2: The category bias is affected by cross-linguistic effects. Hence, for each language learnt by a bilingual child, there is a shift in category biases _away from_ monolinguals of that language, _towards_ monolinguals of the other language.

> H3: The category bias is affected by contextual factors (e.g., culture-specific attentional foci). Hence, bilingual children learning the same combination of languages but in different environmental contexts may exhibit different-sized category biases.

In the present study, we test these hypotheses through a set of three analyses on data from early children's vocabularies to understand the possible origin of the noun bias in young children's word learning.

# Methods
In this section, we describe the broad methodological approach for all three analyses, leaving analysis-specific details to the subsequent sections.
All code is available at [osf.io/8rdwh](https://osf.io/8rdwh/?view_only=a6e76ebe23b6422d8e1f90c1093da9db).

## Vocabulary data
```{r demogs, eval=F}
#| fig.env = "figure", 
#| fig.pos = "t", 
#| fig.align = 'center', 
#| out.width = "240pt",
#| set.cap.width = T, 
#| num.cols.cap = 1, 
#| fig.cap = "Demographics of participants in all three analyses by country and language group. For Analysis 2, ranges indicate children's exposure proportions to English."

knitr::include_graphics("figs/demogs.png", error = FALSE)
```

We obtained CDI data from Wordbank [@frankWordbankOpenRepository2017], an open repository of CDI data including a few bilingual datasets.
These datasets contained vocabulary data and demographic information (e.g., age and sex) for each child.
Each dataset also included language exposure values, standardised as percentages of exposure to each language.
An overview of participant demographics for all analyses can be found at [osf.io/s5mwf](https://osf.io/s5mwf?view_only=a6e76ebe23b6422d8e1f90c1093da9db)<!--is shown in Figure \ref{fig:demogs}-->.
We focused on production data for all analyses. 
<!--The data were retrieved using the `wordbankr` package [@braginskyWordbankrAccessingWordbank2022].-->

## Matching
Several covariates are known to affect early vocabularies, including the child's age and sex [@frankVariabilityConsistencyEarly2021]. 
We statistically controlled for these effects through matching.
For each analysis, we divided participants up into relevant groups (e.g., monolingual English, bilingual English--Spanish etc.).
We then matched participants across groups, with the smallest group of interest as the focal group [i.e., common referent matching, @rassenSimultaneouslyAssessingIntended2011].
We used cardinality matching as the matching method, which selects the largest subset of participants that satisfy balance constraints on covariates using mixed-integer programming; this method provides better sample retention for small sample sizes [@fortinAppliedComparisonLargescale2021]. 
We matched for age and sex as covariates, as well as language exposure when comparing between bilinguals of different language combinations or countries.
We verified the balance of the matched groups by evaluating the standardised mean differences for covariates after matching, using 0.1 as the threshold for balance.
The matched sample provides a fairer comparison between groups, with the trade-off of lower power; we also conducted a sensitivity analysis on the full pre-matched sample to determine if any effects would have differed depending on matching.

## Bias estimation
We divided CDI items into five syntactic categories: nouns, verbs, adjectives, function words (closed-class words), and other (e.g., onomatopoeia and proper nouns); items in the other category were excluded from further analyses.
We then calculated the proportion of items known for each syntactic category (e.g., the number of nouns known divided by the total number of nouns on the CDI), as well as total vocabulary as a proportion of all CDI items.
<!--In this formulation, if a child's word learning were independent of syntactic category, the proportion values for each category would be equal to the total vocabulary proportion. 
If they exhibited a bias _towards_ a syntactic category, they should know more words in that category than expected, and vice versa for a bias _against_ a syntactic category.-->

We can thus calculate a group-level bias by estimating a generalised linear model predicting category proportion as a function of vocabulary proportion, and measuring the area between this curve and the diagonal.
If the curve lay above the diagonal, this area would be positive, indicating a positive bias, while if the curve lay below the diagonal, this area would be negative, indicating a negative bias.
Following previous work [@frankVariabilityConsistencyEarly2021], we used a third-order polynomial model constrained to pass through (0, 0) and (1, 1).
We also estimated confidence intervals by resampling 10,000 times with replacement for each group.

## Permutation testing
To test if groups differed in their syntactic category biases, we conducted permutation testing by shuffling group labels and re-estimating the biases.
For each analysis, we shuffled group labels 10,000 times and re-calculated the between-group differences in bias, allowing for an estimation of the $p$-value of the true observed between-group difference. 
We used a threshold of $\alpha = .05$ for significance.
All $p$-values were adjusted for multiple comparisons using Benjamini--Hochberg correction [@benjaminiControllingFalseDiscovery1995].

# Analysis 1: Varying language combinations
Our first analysis attempted to understand if children with different language combinations would exhibit different category biases.
In particular, we examined if bilinguals differed from monolinguals, and if different types of bilinguals (Mandarin--English and Malay--English) differed from each other, replicating @chaiExtralinguisticModulationEnglish2021.
This set of language combinations is particularly interesting because Mandarin has been suggested to have a positive verb bias and no noun bias, whereas English has a positive noun bias and a negative verb bias [e.g., @frankVariabilityConsistencyEarly2021], and Malay was hypothesised to be more similar to English than Mandarin [@chaiExtralinguisticModulationEnglish2021].
Hence, if syntactic category bias can be cross-linguistically modulated, it should be apparent in Mandarin--English bilinguals, whereas we do not expect the same modulation for Malay--English bilinguals.

## Dataset
```{r}
my_demog <- readRDS(here(INTERMEDIATE_LOC, "my_demog.rds"))
my_matched_cleaned <- readRDS(here(INTERMEDIATE_LOC, "my_matched_cleaned.rds"))
my_demog_matched <- my_matched_cleaned |> 
  select(data_id, age, sex, lang_group_fine) |> distinct()
my_full_cleaned <- readRDS(here(INTERMEDIATE_LOC, "my_full_cleaned.rds"))
```

<!--Can consider making a big ppt table for all 3 analyses?-->
The dataset for this analysis was contributed by @chaiExtralinguisticModulationEnglish2021.
It includes `r nrow(my_demog)` children (`r sum(my_demog$sex == "Male")` boys) from Malaysia aged `r floor(min(my_demog$age))`--`r ceiling(max(my_demog$age))` months.
We thresholded the language exposure values at 10% (i.e., participants with <10% exposure to a language were considered to not be exposed to that language), and divided the participants into groups based on the languages they were exposed to.
Mandarin--Malay bilinguals and Mandarin--Malay--English trilinguals were too few in number and were omitted. 
We used matching over the remaining groups, with Mandarin--English bilinguals as the focal group.[^1]
This resulted in a final sample of `r nrow(my_demog_matched)` children (`r sum(my_demog_matched$sex == "Male")` boys). 

[^1]: Note that the monolingual English group was in fact smaller than the Mandarin--English bilingual group, but matching to that group resulted in a much smaller final sample; hence, we matched to the second-smallest group and accepted a small amount of bias that would result from this decision.

## Results and discussion
```{r}
my_matched_models <- my_matched_cleaned |> 
  nest(data = -c(language, lex_cat, lang_group)) |> 
  mutate(model = map(data, ~ langcog::clm(prop_class ~ I(prop_total^3) + 
                                            I(prop_total^2) + prop_total - 1, 
                                          data = .)),
         predictions = map(model, ~ broom:::augment.lm(., newdata = 
                                                         tibble(prop_total = pts))))

my_matched_preds <- my_matched_models |> 
  select(language, lex_cat, lang_group, predictions) |> 
  unnest(predictions) |> 
  rename(prop_class = .fitted)
```

```{r my_prop}
#| fig.env = "figure", 
#| fig.pos = "t", 
#| fig.align = "center", 
#| fig.height = 5,
#| fig.width = 5,
#| out.width = "220pt",
#| set.cap.width = T, 
#| num.cols.cap = 1, 
#| fig.cap = "Syntactic category composition curves for each language and participant group from the Malaysian sample (Analysis 1). Lines show model fits. The dashed line represents no bias. eng: English; cmn: Mandarin; msa: Malay; mon: monolinguals."

my_prop_plot <- make_prop_plot(my_matched_cleaned, my_matched_preds)
my_prop_plot
```

To illustrate our analytic approach, syntactic category composition curves are shown in Figure \ref{fig:my_prop}, reflecting the proportion of each syntactic category produced by each child as a function of the proportion of all items produced by the child. 
As found in previous work [@frankVariabilityConsistencyEarly2021], children exhibited a negative function word bias across all languages and groups.
Children also showed a positive noun bias in English and a positive verb bias in Mandarin, while the size of the noun and verb biases was close to zero for Malay.

```{r}
my_sampled_areas <- readRDS(here(INTERMEDIATE_LOC, "my_sampled_areas.rds"))

my_observed_areas <- my_matched_cleaned |> 
  nest(data = -c(language, lex_cat, lang_group)) |> 
  mutate(area = map_dbl(data, poly_area)) |> 
  select(-data)

my_full_areas <- my_full_cleaned |> 
  nest(data = -c(language, lex_cat, lang_group)) |> 
  mutate(mean_full = map_dbl(data, poly_area)) |> 
  select(-data)

my_sampled_summary <- my_sampled_areas |> 
  filter(!is.na(area)) |> 
  group_by(language, lang_group, lex_cat) |> 
  summarise(ci_lower = langcog::ci_lower(area),
            ci_upper = langcog::ci_upper(area),
            .groups = "drop") |> 
  left_join(my_observed_areas, by = join_by(language, lang_group, lex_cat)) |> 
  rename(mean = area) |> 
  left_join(my_full_areas, by = join_by(language, lang_group, lex_cat))
```


```{r my_bias}
#| fig.env = "figure", 
#| fig.pos = "t", 
#| fig.align = "center", 
#| fig.height = 5,
#| fig.width = 6,
#| out.width = "240pt",
#| set.cap.width = T, 
#| num.cols.cap = 1, 
#| fig.cap = "Estimated syntactic category biases for each language and participant group from the Malaysian sample (Analysis 1). Ranges indicate bootstrapped 95% confidence intervals. The dashed line represents no bias. Translucent points indicate bias for the full pre-matched sample.\\protect\\footnotemark eng: English; cmn: Mandarin; msa: Malay; mon: monolinguals."

my_bias_plot <- make_bias_plot(my_sampled_summary) +
  scale_colour_manual(values = c("#A9CD66", "#66CDB2", "#B766CD"),
                      guide = guide_legend(reverse = TRUE))
my_bias_plot
```

\footnotetext{In category bias figures, some translucent points appear to be hidden as they coincide almost exactly with the fully opaque points.}

```{r}
# across
my_observed_areas_wide <- my_observed_areas |> 
  pivot_wider(names_from = lang_group,
              values_from = area) |> 
  mutate(engmsa_minus_mon = `eng-msa` - mon,
         engcmn_minus_mon = `eng-cmn` - mon,
         engcmn_minus_engmsa = `eng-cmn` - `eng-msa`)

my_observed_areas_diff <- my_observed_areas_wide |> 
  select(-`eng-msa`, -`eng-cmn`, -mon) |> 
  pivot_longer(cols = c("engmsa_minus_mon", "engcmn_minus_mon", "engcmn_minus_engmsa"), 
               names_to = "comparison", 
               values_to = "area_diff")

my_permuted_areas <- readRDS(here(INTERMEDIATE_LOC, "my_permuted_areas.rds"))

my_permuted_areas_wide <- my_permuted_areas |> 
  pivot_wider(names_from = lang_group,
              values_from = area) |> 
  mutate(engmsa_minus_mon = `eng-msa` - mon,
         engcmn_minus_mon = `eng-cmn` - mon,
         engcmn_minus_engmsa = `eng-cmn` - `eng-msa`)

my_permuted_areas_diff <- my_permuted_areas_wide |> 
  select(-`eng-msa`, -`eng-cmn`, -mon) |> 
  pivot_longer(cols = c("engmsa_minus_mon", "engcmn_minus_mon", "engcmn_minus_engmsa"), 
               names_to = "comparison", 
               values_to = "area_diff")

my_perm_test <- my_permuted_areas_diff |> 
  left_join(my_observed_areas_diff, 
            by = c("language", "lex_cat", "comparison"),
            suffix = c("_perm", "_obs")) |> 
  mutate(obs_beats_perm = abs(area_diff_perm) - abs(area_diff_obs) > 0) |> 
  group_by(language, lex_cat, comparison) |> 
  summarise(obs_p = sum(obs_beats_perm, na.rm = TRUE) / n(),
            .groups = "drop") |> 
  filter(!(language == "Malay" & comparison == "engcmn_minus_mon"),
         !(language == "Mandarin" & comparison == "engmsa_minus_mon"),
         !(language != "English" & comparison == "engcmn_minus_engmsa")) |> 
  arrange(comparison)
```

```{r}
# within
my_observed_areas_wide_within <- my_observed_areas |> 
  pivot_wider(names_from = language,
              values_from = area) |> 
  mutate(`eng-msa` = Malay - English,
         `eng-cmn` = Mandarin - English)

my_observed_areas_diff_within <- my_observed_areas_wide_within |> 
  select(-c(Malay, Mandarin, English)) |> 
  pivot_longer(cols = c("eng-msa", "eng-cmn"), 
               names_to = "comparison", 
               values_to = "area_diff")

my_permuted_areas_wide_within <- my_permuted_areas |> 
  pivot_wider(names_from = language,
              values_from = area) |> 
  mutate(`eng-msa` = Malay - English,
         `eng-cmn` = Mandarin - English)

my_permuted_areas_diff_within <- my_permuted_areas_wide_within |> 
  select(-c(Malay, Mandarin, English)) |> 
  pivot_longer(cols = c("eng-msa", "eng-cmn"), 
               names_to = "comparison", 
               values_to = "area_diff")

my_perm_test_within <- my_permuted_areas_diff_within |> 
  left_join(my_observed_areas_diff_within, 
            by = c("lang_group", "lex_cat", "comparison"),
            suffix = c("_perm", "_obs")) |> 
  mutate(obs_beats_perm = abs(area_diff_perm) - abs(area_diff_obs) > 0) |> 
  group_by(lang_group, lex_cat, comparison) |> 
  summarise(obs_p = sum(obs_beats_perm, na.rm = TRUE) / n(),
            .groups = "drop") |> 
  filter(lang_group == comparison) |> 
  arrange(comparison)
```

```{r}
my_perm_test_full <- bind_rows(my_perm_test, my_perm_test_within) |> 
  mutate(obs_p_adj = p.adjust(obs_p, "BH"))
```

To visualise the sizes of the syntactic category biases, Figure \ref{fig:my_bias} shows model estimates for each language and participant group.
We examined the following comparisons in our permutation test: bilinguals vs corresponding monolinguals, Mandarin--English vs Malay--English bilinguals, and English vs Mandarin/Malay within bilingual groups.
Although visually there appears to be variation across participant groups, there were not in fact any statistically significant differences for any syntactic category in any language (all $p$ ≥ `r min(my_perm_test_full$obs_p_adj) |> papaja::apa_p()`).
No effects were found even in the full pre-matched sample.
In other words, participants with different language backgrounds exhibited similarly-sized category biases for any given language, and bilinguals did not differ in their category biases between their two languages, corroborating and extending findings from @chaiExtralinguisticModulationEnglish2021.[^2] 

[^2]: A similar analysis for comprehension data also did not reveal any significant differences between groups, even for the between-bilinguals condition found to have an effect by @chaiExtralinguisticModulationEnglish2021.

The lack of an effect is nonetheless surprising, since Mandarin and English have been found (in other studies) to have large differences in category bias sizes. 
There are a few possible reasons for this null finding.
First, the category bias effect sizes may have been influenced by language exposure.
Although we controlled for the effect of exposure statistically through matching, it is plausible that greater exposure to one language may entrain a larger modulation of category bias sizes in the other language for a bilingual.
Because children with different exposure distributions were collapsed together in this analysis, it was not possible to determine the contribution of language exposure to category bias sizes.
Second, it is possible that there was not enough power to find a significant effect due to the small sample size.
To address these issues, we shifted to a larger dataset in our second analysis, which allowed us to bin participants by language exposure.

# Analysis 2: Varying language exposures
In this analysis, we examined the effect of language exposure by binning children by language exposure proportions within a large sample of English--Spanish bilinguals.
If category bias were affected by cross-linguistic influence, we should see the largest modulation for children who receive the most exposure to another language.
In addition, across a bilingual child's two languages, we should see the biggest difference in bias sizes for children with the biggest imbalance in language exposure.

## Dataset
```{r}
us_demog <- readRDS(here(INTERMEDIATE_LOC, "us_demog.rds"))
us_matched_cleaned <- readRDS(here(INTERMEDIATE_LOC, "us_matched_cleaned.rds"))
us_demog_matched <- us_matched_cleaned |> 
  select(data_id, age, sex, lang_group) |> distinct()
us_full_cleaned <- readRDS(here(INTERMEDIATE_LOC, "us_full_cleaned.rds"))
```

The dataset for this analysis included data from @hoffDualLanguageExposure2012, @hoffWhatExplainsCorrelation2018, and @marchmanLanguagespecificNatureGrammatical2004 on English--Spanish bilingual children.
It includes `r nrow(us_demog)` children (`r sum(us_demog$sex == "Male")` boys) from the US aged `r floor(min(us_demog$age))`--`r ceiling(max(us_demog$age))` months.
We binned participants into five bins by language exposure, and matched participants across bins. 
This resulted in a final sample of `r nrow(us_demog_matched)` children (`r sum(us_demog_matched$sex == "Male")` boys). 

## Results and discussion
```{r}
us_sampled_areas <- readRDS(here(INTERMEDIATE_LOC, "us_sampled_areas.rds"))

us_observed_areas <- us_matched_cleaned |> 
  nest(data = -c(language, lex_cat, lang_group)) |> 
  mutate(area = map_dbl(data, poly_area)) |> 
  select(-data)

us_full_areas <- us_full_cleaned |> 
  nest(data = -c(language, lex_cat, lang_group)) |> 
  mutate(mean_full = map_dbl(data, poly_area)) |> 
  select(-data)

us_sampled_summary <- us_sampled_areas |> 
  filter(!is.na(area)) |> 
  group_by(language, lang_group, lex_cat) |> 
  summarise(ci_lower = langcog::ci_lower(area),
            ci_upper = langcog::ci_upper(area),
            .groups = "drop") |> 
  left_join(us_observed_areas, by = join_by(language, lang_group, lex_cat)) |> 
  rename(mean = area) |> 
  left_join(us_full_areas, by = join_by(language, lang_group, lex_cat))
```

```{r us_bias}
#| fig.env = "figure", 
#| fig.pos = "t", 
#| fig.align = "center", 
#| fig.height = 5.5,
#| fig.width = 6,
#| out.width = "240pt",
#| set.cap.width = T, 
#| num.cols.cap = 1, 
#| fig.cap = "Estimated syntactic category biases for each language and English exposure proportion from the US sample (Analysis 2). Ranges indicate bootstrapped 95% confidence intervals. The dashed line represents no bias. Translucent points indicate bias for the full pre-matched sample."

us_bias_plot <- make_bias_plot(us_sampled_summary) +
  scale_colour_viridis_d(guide = guide_legend(reverse = TRUE))
us_bias_plot
```

```{r}
# across
us_observed_areas_wide <- us_observed_areas |> 
  pivot_wider(names_from = lang_group,
              values_from = area) |> 
  mutate(across(-c("lex_cat", "language"), \(x) {x - `(80,100]`}))

us_observed_areas_diff <- us_observed_areas_wide |> 
  pivot_longer(cols = -c("lex_cat", "language"), 
               names_to = "comparison", 
               values_to = "area_diff")

us_permuted_areas <- readRDS(here(INTERMEDIATE_LOC, "us_permuted_areas.rds"))

us_permuted_areas_wide <- us_permuted_areas |> 
  pivot_wider(names_from = lang_group,
              values_from = area) |> 
  # ungroup() |> 
  mutate(across(-c("lex_cat", "language", "sample"), \(x) {x - `(80,100]`}))

us_permuted_areas_diff <- us_permuted_areas_wide |> 
  pivot_longer(cols = -c("lex_cat", "language", "sample"), 
               names_to = "comparison", 
               values_to = "area_diff")

us_perm_test <- us_permuted_areas_diff |> 
  left_join(us_observed_areas_diff, 
            by = c("language", "lex_cat", "comparison"),
            suffix = c("_perm", "_obs")) |> 
  mutate(obs_beats_perm = abs(area_diff_perm) - abs(area_diff_obs) > 0) |> 
  group_by(language, lex_cat, comparison) |> 
  summarise(obs_p = sum(obs_beats_perm, na.rm = TRUE) / n(),
            .groups = "drop") |> 
  filter(comparison != "(80,100]") |> 
  arrange(comparison)
```

```{r}
# within
us_observed_areas_wide_within <- us_observed_areas |> 
  mutate(exp_eng = ifelse(language == "English",
                          as.character(lang_group),
                          case_when(
                            lang_group == "[0,20]" ~ "(80,100]",
                            lang_group == "(20,40]" ~ "(60,80]",
                            lang_group == "(40,60]" ~ "(40,60]",
                            lang_group == "(60,80]" ~ "(20,40]",
                            lang_group == "(80,100]" ~ "[0,20]",
                          ))) |> 
  select(-lang_group) |> 
  pivot_wider(names_from = language,
              values_from = area) |> 
  # ungroup() |> 
  mutate(spaeng = Spanish - English)

us_observed_areas_diff_within <- us_observed_areas_wide_within |> 
  select(-Spanish, -English) |> 
  pivot_longer(cols = -c("lex_cat", "exp_eng"), 
               names_to = "comparison", 
               values_to = "area_diff")

us_permuted_areas_wide_within <- us_permuted_areas |> 
  mutate(exp_eng = ifelse(language == "English",
                          as.character(lang_group),
                          case_when(
                            lang_group == "[0,20]" ~ "(80,100]",
                            lang_group == "(20,40]" ~ "(60,80]",
                            lang_group == "(40,60]" ~ "(40,60]",
                            lang_group == "(60,80]" ~ "(20,40]",
                            lang_group == "(80,100]" ~ "[0,20]",
                          ))) |> 
  select(-lang_group) |> 
  pivot_wider(names_from = language,
              values_from = area) |> 
  # ungroup() |> 
  mutate(spaeng = Spanish - English)

us_permuted_areas_diff_within <- us_permuted_areas_wide_within |> 
  select(-Spanish, -English) |> 
  pivot_longer(cols = -c("lex_cat", "exp_eng", "sample"), 
               names_to = "comparison", 
               values_to = "area_diff")

us_perm_test_within <- us_permuted_areas_diff_within |> 
  left_join(us_observed_areas_diff_within, 
            by = c("exp_eng", "lex_cat", "comparison"),
            suffix = c("_perm", "_obs")) |> 
  mutate(obs_beats_perm = abs(area_diff_perm) - abs(area_diff_obs) > 0) |> 
  group_by(exp_eng, lex_cat, comparison) |> 
  summarise(obs_p = sum(obs_beats_perm, na.rm = TRUE) / n(),
            .groups = "drop") |> 
  arrange(comparison)
```

```{r}
us_perm_test_full <- bind_rows(us_perm_test, us_perm_test_within) |> 
  mutate(obs_p_adj = p.adjust(obs_p, "BH"))
```

Figure \ref{fig:us_bias} shows model estimates for each language and participant group.
We examined the following comparisons in our permutation test: (80,100] vs other exposure bins for each language, and Spanish vs English for each corresponding exposure bin (i.e., [0,20] for English corresponds to (80,100] for Spanish).
Six comparisons survived the correction for multiple comparisons.
Children exposed to (80,100] percent English had more negative function word biases ($p$ = `r us_perm_test_full |> filter(language == "English", lex_cat == "function_words", comparison == "[0,20]") |> pull(obs_p_adj) |> papaja::apa_p()`), more negative adjective biases ($p$ = `r us_perm_test_full |> filter(language == "English", lex_cat == "adjectives", comparison == "[0,20]") |> pull(obs_p_adj) |> papaja::apa_p()`), and more positive noun biases ($p$ = `r us_perm_test_full |> filter(language == "English", lex_cat == "nouns", comparison == "[0,20]") |> pull(obs_p_adj) |> papaja::apa_p()`) than children exposed to [0,20] percent English.
Children exposed to (80,100] percent English also had more negative adjective biases ($p$ = `r us_perm_test_full |> filter(language == "English", lex_cat == "adjectives", comparison == "(20,40]") |> pull(obs_p_adj) |> papaja::apa_p()`) and more positive noun biases ($p$ = `r us_perm_test_full |> filter(language == "English", lex_cat == "nouns", comparison == "(20,40]") |> pull(obs_p_adj) |> papaja::apa_p()`) than children exposed to (20,40] percent English.
Finally, children exposed to (80,100] percent English had more negative function word biases in English than in Spanish ($p$ < .001).
Nine other comparisons were significant prior to correction but did not survive the correction (all $p \in$ [`r us_perm_test_full |> filter(obs_p_adj > .05, obs_p < .05) |> pull(obs_p_adj) |> min() |> papaja::apa_p()`, `r us_perm_test_full |> filter(obs_p_adj > .05, obs_p < .05) |> pull(obs_p_adj) |> max() |> papaja::apa_p()`]).
Six of these comparisons were also significant in the full pre-matched sample.

As hypothesised, we found significant modulation of category biases in English for children exposed to more Spanish---i.e., the category biases of children exposed to >80% Spanish had _English_ category biases that were more like those of Spanish, in comparison to children exposed to ≤20% Spanish; this finding supports H2 (cross-linguistic factors).
In addition, children with the biggest imbalance in language exposure (>80% English, ≤20% Spanish) showed the biggest difference in category biases across their two languages, supporting H1 (intra-linguistic factors).
Interestingly, these effects were asymmetric---children exposed to more English did not show any modulation of their category biases in Spanish, and children exposed to mostly Spanish did not show a big difference in category biases across their languages.
One reason for this asymmetry may be the statuses of the two languages---in the US, English is the majority language while Spanish is a minority language.
The difference in language status may drive contextual differences in which these languages appear---for example, Spanish may be spoken more at home, while English may be spoken more in community settings.
Thus, the distributional properties of Spanish may not change regardless of language exposure, while those for English may shift from more idiolectal at lower exposure levels to more standard at higher exposure levels.
<!--Probably need a tigher linking hypothesis between contextual differences and observed category bias asymmetry-->
To understand the possible role of context, we turn to varying the environmental context in our third analysis.

# Analysis 3: Varying environmental contexts
In this analysis, we examine the effect of environmental context by comparing children acquiring the same pairs of languages but in different countries.
If environmental factors (e.g., language status) affect category bias, we should observe differences in bias sizes for children in different countries, even if they are acquiring the same pair of languages, once factors such as age and language exposure are controlled for.

## Dataset
```{r}
ox_demog <- readRDS(here(INTERMEDIATE_LOC, "ox_demog.rds")) |> 
  filter(other_lang %in% c("German", "Germany",
                           "Dutch", "Netherlands",
                           "Spain", "Spanish"))
ox_matched_cleaned <- readRDS(here(INTERMEDIATE_LOC, "ox_matched_cleaned.rds"))
ox_demog_matched <- ox_matched_cleaned |> 
  select(data_id, age, sex, other_lang, lang_group) |> distinct()
ox_full_cleaned <- readRDS(here(INTERMEDIATE_LOC, "ox_full_cleaned.rds"))
```

The dataset for this analysis included data from @siowDoubleItVocabulary2023 on bilingual children in the UK and Continental Europe.
We chose a subset of language combinations that had data from both the UK and another European country: English--Dutch (UK/the Netherlands), English--German (UK/Germany), and English--Spanish (UK/Spain).
The data include `r nrow(ox_demog)` children (`r sum(ox_demog$sex == "Male")` boys) aged `r floor(min(ox_demog$age))`--`r ceiling(max(ox_demog$age))` months.
As in Analysis 1, we thresholded language exposure values at 10% and grouped participants by language combination and country of origin, performing matching within each language combination.
We also included the small set of UK monolingual English children for completeness.
This resulted in a final sample of `r nrow(ox_demog_matched)` children (`r sum(ox_demog_matched$sex == "Male")` boys). 

## Results and discussion
```{r}
ox_sampled_areas <- readRDS(here(INTERMEDIATE_LOC, "ox_sampled_areas.rds"))

ox_observed_areas <- ox_matched_cleaned |> 
  nest(data = -c(language, lex_cat, lang_group)) |> 
  mutate(area = map_dbl(data, poly_area)) |> 
  select(-data)

ox_full_areas <- ox_full_cleaned |> 
  nest(data = -c(language, lex_cat, lang_group)) |> 
  mutate(mean_full = map_dbl(data, poly_area)) |> 
  select(-data)

ox_sampled_summary <- ox_sampled_areas |> 
  filter(!is.na(area)) |> 
  group_by(language, lang_group, lex_cat) |> 
  summarise(ci_lower = langcog::ci_lower(area),
            ci_upper = langcog::ci_upper(area),
            .groups = "drop") |> 
  left_join(ox_observed_areas, by = join_by(language, lang_group, lex_cat)) |> 
  rename(mean = area) |> 
  left_join(ox_full_areas, by = join_by(language, lang_group, lex_cat)) |> 
  mutate(language = case_when(
    language == "Germany" ~ "German",
    language == "Netherlands" ~ "Dutch",
    language == "Spain" ~ "Spanish",
    .default = language
  ) |> fct_relevel(c("English", "Dutch", "German", "Spanish")) |> fct_rev(),
  lang_group = lang_group |> 
    fct_relevel(c("English", "Dutch", "Netherlands",
                  "German", "Germany", "Spanish", "Spain")) |> 
    fct_recode(`eng-gb` = "English", `nld-gb` = "Dutch", `nld-nd` = "Netherlands",
               `deu-gb` = "German", `deu-de` = "Germany", 
               `esp-gb` = "Spanish", `esp-sp` = "Spain"))
```

```{r ox_bias}
#| fig.env = "figure", 
#| fig.pos = "t", 
#| fig.align = "center", 
#| fig.height = 5.5,
#| fig.width = 6,
#| out.width = "240pt",
#| set.cap.width = T, 
#| num.cols.cap = 1, 
#| fig.cap = "Estimated syntactic category biases for each language and exposure proportion from the European sample (Analysis 3). Ranges indicate bootstrapped 95% confidence intervals. The dashed line represents no bias. Translucent points indicate bias for the full pre-matched sample. eng: English; nld: Dutch; deu: German; spa: Spanish; mon: monolinguals."

ox_bias_data <- ox_sampled_summary |> 
  mutate(Country = ifelse(str_sub(lang_group, 5, 6) == "gb",
                          "UK", "EU") |> 
           fct_relevel("UK", "EU") |> fct_rev(),
         lang_group = str_sub(lang_group, 1, 3),
         lang_group = case_when(
           lang_group == "eng" ~ "mon",
           lang_group == "nld" ~ "eng-nld",
           lang_group == "deu" ~ "eng-deu",
           lang_group == "esp" ~ "eng-spa"
         ) |> 
           fct_relevel(c("mon", "eng-nld", 
                         "eng-deu", "eng-spa")) |> 
           fct_rev())
ox_bias_eng <- make_bias_plot(ox_bias_data |> filter(language == "English"),
                              shape_opt = Country) +
  scale_color_brewer(type = "qual", palette = "Set2") +
  scale_shape_manual(values = c(15, 16), 
                     guide = guide_legend(reverse = TRUE)) +
  coord_cartesian(xlim = c(-0.15, 0.075)) +
  theme(strip.background = element_blank(),
        strip.text = element_blank())
ox_bias_oth <- make_bias_plot(ox_bias_data |> filter(language != "English"),
                              shape_opt = Country) +
  scale_color_brewer(type = "qual", palette = "Set2") +
  scale_shape_manual(values = c(15, 16), 
                     guide = guide_legend(reverse = TRUE)) +
  coord_cartesian(xlim = c(-0.15, 0.075)) +
  theme(legend.position = "none")

ox_bias_twocol <- (ox_bias_eng | ox_bias_oth) + 
  plot_layout(axes = "collect", guides = "collect")
ox_bias_twocol
```

```{r}
# English
ox_observed_areas_wide_eng <- ox_observed_areas |> 
  filter(language == "English") |> 
  pivot_wider(names_from = lang_group,
              values_from = area) |> 
  mutate(engdeu_de_vs_gb = Germany - German,
         engnld_nd_vs_gb = Netherlands - Dutch,
         engspa_es_vs_gb = Spain - Spanish,
         across(-c("lex_cat", "language"), \(x) {x - English}))

ox_observed_areas_diff_eng <- ox_observed_areas_wide_eng |> 
  pivot_longer(cols = -c("lex_cat", "language"), 
               names_to = "comparison", 
               values_to = "area_diff")

ox_permuted_areas_eng <- readRDS(here(INTERMEDIATE_LOC, "ox_permuted_areas_eng.rds"))

ox_permuted_areas_wide_eng <- ox_permuted_areas_eng |> 
  filter(language == "English") |> 
  pivot_wider(names_from = lang_group,
              values_from = area) |> 
  mutate(engdeu_de_vs_gb = Germany - German,
         engnld_nd_vs_gb = Netherlands - Dutch,
         engspa_es_vs_gb = Spain - Spanish,
         across(-c("lex_cat", "language", "sample"), \(x) {x - English}))

ox_permuted_areas_diff_eng <- ox_permuted_areas_wide_eng |> 
  pivot_longer(cols = -c("lex_cat", "language", "sample"), 
               names_to = "comparison", 
               values_to = "area_diff")

ox_perm_test_eng <- ox_permuted_areas_diff_eng |> 
  left_join(ox_observed_areas_diff_eng, 
            by = c("language", "lex_cat", "comparison"),
            suffix = c("_perm", "_obs")) |> 
  mutate(obs_beats_perm = abs(area_diff_perm) - abs(area_diff_obs) > 0) |> 
  group_by(language, lex_cat, comparison) |> 
  summarise(obs_p = sum(obs_beats_perm, na.rm = TRUE) / n(),
            .groups = "drop") |> 
  filter(comparison != "English") |> 
  arrange(comparison)
```

```{r}
# other language
ox_observed_areas_wide_oth <- ox_observed_areas |> 
  mutate(language = case_when(
    language == "Germany" ~ "German",
    language == "Netherlands" ~ "Dutch",
    language == "Spain" ~ "Spanish",
    .default = language
  ) |> fct_relevel(c("English", "Dutch", "German", "Spanish")) |> fct_rev(),
  lang_group = ifelse(lang_group %in% c("German", "Dutch", "Spanish"), "UK", lang_group)) |> 
  filter(language %in% c("German", "Dutch", "Spanish")) |> 
  pivot_wider(names_from = lang_group,
              values_from = area) |> 
  mutate(across(-c("lex_cat", "language"), \(x) {x - UK}))

ox_observed_areas_diff_oth <- ox_observed_areas_wide_oth |> 
  pivot_longer(cols = -c("lex_cat", "language"), 
               names_to = "comparison", 
               values_to = "area_diff") |> 
  filter(!is.na(area_diff), comparison != "UK")

ox_permuted_areas_oth <- readRDS(here(INTERMEDIATE_LOC, "ox_permuted_areas_oth.rds"))

ox_permuted_areas_wide_oth <- ox_permuted_areas_oth |> 
  # filter(language %in% c("German", "Dutch", "Spanish")) |> 
  mutate(lang_group = ifelse(language == lang_group, "UK", lang_group)) |> 
  filter(!is.na(area)) |> 
  pivot_wider(names_from = lang_group,
              values_from = area) |> 
  mutate(across(-c("lex_cat", "language", "sample"), \(x) {x - UK}))

ox_permuted_areas_diff_oth <- ox_permuted_areas_wide_oth |> 
  pivot_longer(cols = -c("lex_cat", "language", "sample"), 
               names_to = "comparison", 
               values_to = "area_diff") |> 
  filter(!is.na(area_diff), comparison != "UK")

ox_perm_test_oth <- ox_permuted_areas_diff_oth |> 
  left_join(ox_observed_areas_diff_oth, 
            by = c("language", "lex_cat", "comparison"),
            suffix = c("_perm", "_obs")) |> 
  mutate(obs_beats_perm = abs(area_diff_perm) - abs(area_diff_obs) > 0) |> 
  group_by(language, lex_cat, comparison) |> 
  summarise(obs_p = sum(obs_beats_perm, na.rm = TRUE) / n(),
            .groups = "drop") |> 
  filter(comparison != "UK") |> 
  arrange(comparison)
```

```{r}
ox_perm_test_full <- bind_rows(ox_perm_test_eng, ox_perm_test_oth) |> 
  mutate(obs_p_adj = p.adjust(obs_p, "BH"))
```

Figure \ref{fig:ox_bias} shows model estimates for each language and participant group.
We examined comparisons between children from different countries within each language combination<!-- (e.g., UK vs Germany for English--German children)-->.
For completeness, we also compared English category biases for all groups against the UK monolingual English group, although the imbalance in group size and other covariates means that these comparisons may be less reliable.
Only one comparison survived the correction for multiple comparisons: function words in German for children from the UK vs Germany ($p$ `r ox_perm_test_full |> filter(language == "German", lex_cat == "function_words") |> pull(obs_p_adj) |> papaja::apa_p()`). 
Three other comparisons were significant prior to correction but did not survive the correction: function words ($p$ = `r ox_perm_test_full |> filter(language == "Spanish", lex_cat == "function_words") |> pull(obs_p_adj) |> papaja::apa_p()`) and nouns ($p$ = `r ox_perm_test_full |> filter(language == "Spanish", lex_cat == "nouns") |> pull(obs_p_adj) |> papaja::apa_p()`) in Spanish for children from the UK vs Spain, and nouns in German for children from the UK vs Germany ($p$ = `r ox_perm_test_full |> filter(language == "German", lex_cat == "nouns") |> pull(obs_p_adj) |> papaja::apa_p()`).
The full pre-matched dataset also had two significant comparisons that were not significant after matching: function words and nouns in Spanish for children from the UK vs Spain.

As hypothesised, we found an effect of environmental context, such that children acquiring English and German exhibited different function word biases depending on whether they were growing up in the UK or in Germany, supporting H3 (contextual factors).
The lack of other significant comparisons may again be due to a lack of power; a larger sample may be able to uncover smaller effects that the current sample cannot.
It may also be the case that the effect depends on the specific language combination, such that there is an interaction between the linguistic and contextual factors on category bias variation.

# General discussion
Across our three analyses, we found evidence that syntactic category biases could vary across a bilingual's two languages, and that these biases could be modulated cross-linguistically (Analysis 2), supporting H1 and H2---category biases are affected by intra- and inter-language factors. 
We also found that category biases were sometimes influenced by children's environmental context (Analysis 3), supporting H3---category biases are affected by contextual factors. 
Taken together, the supported hypotheses suggest that linguistic and contextual factors both separately play a role in shaping the syntactic composition of children's early vocabularies.
This finding also provides an imperative for language acquisition researchers to be more thorough and precise with their description of the language learning environment of their study populations, since background variables other than language may affect their language outcomes [see @titoneRethinkingMultilingualExperience2023].

Our results also suggested that factors affecting category biases were not uniform across language combinations (Analyses 1 and 3).
This observation is perhaps unsurprising---since some language pairs have very similar category bias sizes even for monolinguals, any cross-linguistic influence would not be apparent, whereas other language pairs have very divergent bias sizes, permitting the observation of cross-linguistic influence. 
This result also emphasises the importance of surveying a broader set of language combinations in bilingualism research, since some true effects may not be elicitable in particular language pairs. 
<!-- For example, if English and French have similarly sized syntactic category biases, the influence of one language on the other would not be detectable. -->
A majority of bilingualism research has focused on bilinguals acquiring English and another language (including the present work!), which does not comprehensively cover the possible relationships between a bilingual's two languages; a much broader scope of investigation would be necessary to understand the generalisability of various effects related to bilingualism.

Methodologically, the analytic method used in this work provided a means to directly investigate the over- or under-representation of syntactic categories in children's early vocabularies.
<!-- This method allowed for a measurement that has a closer link with the construct of interest---that is, whether particular syntactic categories are privileged in children's word learning processes---since the number of potentially learnable words is controlled for. -->
However, this approach also required non-parametric statistics, since the bias sizes must be estimated from whole samples (rather than individual children); this requirement in turn meant that we had to control for covariates via matching (instead of direct modelling).
As matching is typically constrained by the subgroup with the smallest size, our approach may have limited the power of our analyses.
Future work should involve larger sample sizes in order to improve the ability of such analyses to detect true effects, although we acknowledge that the task of collecting data from young bilinguals is often difficult as population sizes tend to be smaller than other potential populations of interest.

Nonetheless, the current set of analyses represent an attempt at disentangling potential factors contributing to differential learning of words in young children, providing evidence that linguistic and contextual factors are both important to consider in language acquisition.
These results also show the importance of studying bilingual populations, since any theory of language learning must be able to account for the simultaneous acquisition of multiple languages.
Notably, language acquisition research on monolinguals is much more common than that on multilinguals, even though many more children grow up in multilingual rather than monolingual settings [@kiddHowDiverseChild2022].
In fact, studying bilinguals may afford approaches that are not possible with monolinguals, as evidenced by the present work.
We anticipate that an increasing emphasis on understudied language settings, combined with increasing statistical sophistication and increasing open science practices, will serve to improve the generalisability, accuracy, and rigour of our theories of language acquisition.

\pagebreak
# Acknowledgements
We would like to thank the data contributors of Wordbank, whose efforts towards open science have made this research possible.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
